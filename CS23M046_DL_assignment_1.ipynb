{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRJaw4bwl-m_"
      },
      "source": [
        "# Question 1: Download dataset and plot each class figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Plr2ip_S2Nk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "(x_train,y_train),(x_test,y_test)= fashion_mnist.load_data()\n",
        "x_train=x_train/255\n",
        "x_test=x_test/255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    plt.figure(figsize=(1, 1))\n",
        "\n",
        "    # Find the index of the first image with the current class label\n",
        "    # np.where returns many tuples where y_train==i from this choose first tuple then from this tuple first element\n",
        "    # so np.where(y_train == i)[0][0] will retun index\n",
        "\n",
        "    index = np.where(y_train == i)[0][0]\n",
        "\n",
        "    # Plot the sample image\n",
        "\n",
        "    plt.imshow(x_train[index],cmap='gray')\n",
        "    plt.title(\"Class \" + str(i))\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "# # Initialize wandb\n",
        "# wandb.init(project='DL_Assignment_1_CS23M046', name='Sample Images Classes')\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "# x_train = x_train / 255\n",
        "# x_test = x_test / 255\n",
        "\n",
        "# for i in range(10):\n",
        "#     plt.figure(figsize=(1, 1))\n",
        "\n",
        "#     # Find the index of the first image with the current class label\n",
        "#     index = np.where(y_train == i)[0][0]\n",
        "\n",
        "#     # Log the image along with its class label using wandb\n",
        "#     wandb.log({f\"Class_{i}\": [wandb.Image(x_train[index], caption=f'Class {i}')]})\n",
        "\n",
        "#     # Plot the sample image\n",
        "#     plt.imshow(x_train[index], cmap='gray')\n",
        "#     plt.title(\"Class \" + str(i))\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# wandb.finish()\n"
      ],
      "metadata": {
        "id": "iSs0qiBrFzFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rStP2S0H86ku"
      },
      "outputs": [],
      "source": [
        "# # Choose number of layer\n",
        "# num_layers= 3\n",
        "\n",
        "# # Choose number of nuerons per each layer\n",
        "# num_neurons= 64\n",
        "\n",
        "# num_classes =10\n",
        "\n",
        "\n",
        "# X = x_train\n",
        "\n",
        "# # Reshaping (60000,28,28) to (60000,784)\n",
        "# X = X.reshape(X.shape[0], -1)\n",
        "\n",
        "\n",
        "# n_features=X.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weight Initialization Methods:"
      ],
      "metadata": {
        "id": "S1Z9mkFQuKS3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "22LN0siotHA_"
      },
      "outputs": [],
      "source": [
        "def randomweightsandbias(num_layers,num_neurons,n_features,num_classes):\n",
        "    # Initialize Random weights and biases\n",
        "\n",
        "    # These are W[0] and b[0] which are not in use\n",
        "\n",
        "    W = [np.random.randn(num_neurons,n_features)]\n",
        "    b = [np.random.randn(num_neurons,1)]\n",
        "\n",
        "    # Here initializing W[1] and b[1]\n",
        "\n",
        "    W.append(np.random.randn(num_neurons,n_features))  # Weight matrix for input layer\n",
        "    b.append(np.random.randn(num_neurons,1))# Bias vector for input layer\n",
        "\n",
        "    # Initializing intermidiate W and b\n",
        "\n",
        "    for _ in range(num_layers - 2):\n",
        "      W.append(np.random.randn(num_neurons, num_neurons))\n",
        "      b.append(np.random.randn(num_neurons,1))\n",
        "\n",
        "    # Initializing output layer W and b\n",
        "\n",
        "    W.append(np.random.randn(num_classes,num_neurons))  # Weight matrix for output layer\n",
        "    b.append(np.random.randn(num_classes,1))  # Bias vector for output layer\n",
        "\n",
        "    return W,b\n",
        "\n",
        "def xavier_init_weights(n_in, n_out):\n",
        "    limit = np.sqrt(6.0 / (n_in + n_out))\n",
        "    return np.random.uniform(-limit, limit, size=(n_in, n_out))\n",
        "\n",
        "def xavier_init_bias(n_out):\n",
        "    limit = np.sqrt(6.0 / n_out)\n",
        "    return np.random.uniform(-limit, limit, size=(n_out, 1))\n",
        "\n",
        "def xaviorweightsandbias(num_layers,num_neurons,n_features,num_classes):\n",
        "\n",
        "\n",
        "    # These are W[0] and b[0] which are not in use\n",
        "\n",
        "    W = [xavier_init_weights(num_neurons,n_features)]\n",
        "    b = [xavier_init_bias(num_neurons)]\n",
        "\n",
        "    # Here initializing W[1] and b[1]\n",
        "\n",
        "    W.append(xavier_init_weights(num_neurons,n_features))  # Weight matrix for input layer\n",
        "    b.append(xavier_init_bias(num_neurons))# Bias vector for input layer\n",
        "\n",
        "    # Initializing intermidiate W and b\n",
        "\n",
        "    for _ in range(num_layers - 2):\n",
        "      W.append(xavier_init_weights(num_neurons, num_neurons))\n",
        "      b.append(xavier_init_bias(num_neurons))\n",
        "\n",
        "    # Initializing output layer W and b\n",
        "\n",
        "    W.append(xavier_init_weights(num_classes,num_neurons))  # Weight matrix for output layer\n",
        "    b.append(xavier_init_bias(num_classes))  # Bias vector for output layer\n",
        "\n",
        "\n",
        "    return W,b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jMiJ6t_uPcoy"
      },
      "outputs": [],
      "source": [
        "def initializeWeightandbias(initialize_method,num_layers,num_neurons,n_features,num_classes):\n",
        "\n",
        "    if(initialize_method=='random'):\n",
        "      return randomweightsandbias(num_layers,num_neurons,n_features,num_classes)\n",
        "\n",
        "    elif(initialize_method=='Xavier'):\n",
        "       return xaviorweightsandbias(num_layers,num_neurons,n_features,num_classes)\n",
        "\n",
        "\n",
        "\n",
        "# initialize_method ='Xavior'\n",
        "\n",
        "# W,b = initializeWeightandbias(initialize_method,3,4,728,10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-nHti9AJAzP9"
      },
      "outputs": [],
      "source": [
        "def sigmoid(data1):\n",
        "\n",
        "  # As for high values or low values of data1 issue of overflow so to overcome that used clip\n",
        "\n",
        "  sigmoid_x = 1 /(1+ np.exp(np.clip(-data1, -600, 600)))\n",
        "\n",
        "  return sigmoid_x\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def softmax(data2):\n",
        "    e_x =  np.exp(np.clip(data2, -400, 400))\n",
        "    return e_x / np.sum(e_x)\n",
        "\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "\n",
        "  sigmoid_x = 1 /(1+ np.exp(np.clip(-x, -600, 600)))\n",
        "  return sigmoid_x * (1 - sigmoid_x)\n",
        "\n",
        "\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "def one_hot_encoding(y,num_classes):\n",
        "    encoded = np.zeros((num_classes,1))\n",
        "    encoded[y] = 1\n",
        "\n",
        "    return encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAtNIHra3xTy"
      },
      "source": [
        "# Question 2: Feedforward Nueral Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zyOd5I5f3v03"
      },
      "outputs": [],
      "source": [
        "def feedforwardnn(data,num_layers,num_neurons,W,b,activation_function):\n",
        "\n",
        "\n",
        "  n_features =len(data)\n",
        "\n",
        "  a=[]\n",
        "  h=[]\n",
        "\n",
        "  a.append(data)\n",
        "  h.append(data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for k in range(1, num_layers):\n",
        "\n",
        "    a.append(b[k] + (W[k] @ h[k-1]))\n",
        "\n",
        "\n",
        "\n",
        "    if(activation_function==\"ReLU\"):\n",
        "      h.append(relu(a[k]))\n",
        "    if(activation_function==\"sigmoid\"):\n",
        "      h.append(sigmoid(a[k]))\n",
        "    if(activation_function==\"tanh\"):\n",
        "      h.append(tanh(a[k]))\n",
        "\n",
        "\n",
        "\n",
        "  a.append(b[num_layers] + (W[num_layers] @ h[num_layers-1]))\n",
        "\n",
        "\n",
        "  probability_distribution=(softmax(a[num_layers]))\n",
        "\n",
        "\n",
        "\n",
        "  return a,h,probability_distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VipfC8sY8v8v"
      },
      "outputs": [],
      "source": [
        "#Checking for first data point\n",
        "# a,h,y_hat=feedforwardnn(X[0].reshape(784,1),num_layers,num_neurons,W,b,\"ReLU\")\n",
        "# print(y_hat)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bj4z_2GQHQk"
      },
      "source": [
        "# Question 3: Backpropogation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pFsk7qA-QFqH"
      },
      "outputs": [],
      "source": [
        "def backward_propagation(inputs,h,a,W,b,actual_y,predicted_y,num_layers,num_classes,activation_function,loss_method):\n",
        "\n",
        "    grad_with_a=[np.zeros_like(a[i]) for i in range(num_layers + 1)]\n",
        "    grad_with_w=[np.zeros_like(W[i]) for i in range(num_layers + 1)]\n",
        "    #q=[np.zeros_like(W[i]) for i in range(num_layers + 1)]\n",
        "    grad_with_b=[np.zeros_like(b[i]) for i in range(num_layers + 1)]\n",
        "\n",
        "\n",
        "\n",
        "    grad_with_h=[np.zeros_like(h[i]) for i in range(num_layers )]\n",
        "    grad_with_h[0]=inputs\n",
        "\n",
        "    y_encoded =  one_hot_encoding(actual_y,num_classes)\n",
        "\n",
        "    if(loss_method=='cross_entropy'):\n",
        "      grad_with_a[num_layers] = (predicted_y-y_encoded )\n",
        "\n",
        "    else:\n",
        "      grad_with_a[num_layers] = np.multiply(-2 * (predicted_y-y_encoded ),np.multiply(predicted_y,(1-predicted_y)))\n",
        "\n",
        "\n",
        "\n",
        "    for k in range(num_layers, 0, -1):\n",
        "\n",
        "\n",
        "\n",
        "        # Compute gradiant with respect to parameters\n",
        "\n",
        "\n",
        "\n",
        "        grad_with_w[k]=(grad_with_a[k] @ (h[k-1].T))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        grad_with_b[k]=grad_with_a[k]\n",
        "\n",
        "        # Compute gradian with respect to layer below\n",
        "\n",
        "        grad_with_h[k-1] = W[k].T @ grad_with_a[k]\n",
        "\n",
        "        # Compute gradian with respect to layer below (Pre activation)\n",
        "\n",
        "        if(activation_function==\"ReLU\"):\n",
        "\n",
        "          grad_with_a[k-1] = grad_with_h[k-1] * relu_derivative(a[k-1])\n",
        "\n",
        "        elif(activation_function==\"sigmoid\"):\n",
        "\n",
        "          grad_with_a[k-1] = grad_with_h[k-1] * sigmoid_derivative(a[k-1])\n",
        "\n",
        "        elif(activation_function==\"tanh\"):\n",
        "\n",
        "          grad_with_a[k-1] = grad_with_h[k-1] * tanh_derivative(a[k-1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return grad_with_w, grad_with_b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# grad_with_w, grad_with_b= backward_propagation(X[0].reshape(784,1),h,a,W,b,y_train[0],y_hat,\"ReLU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy and Loss Function"
      ],
      "metadata": {
        "id": "uUKA-3OpSBmJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "T7bH-2yTCfSd"
      },
      "outputs": [],
      "source": [
        "def LossAccuracy(x_train,y_train,alpha,W,b,batch_size,activation_function,num_layers,n_features,num_classes,num_neurons,loss_method) :\n",
        "\n",
        "\n",
        "    X = x_train.reshape(x_train.shape[0], -1)\n",
        "\n",
        "     # For Training error\n",
        "\n",
        "    #Initial loss is 0\n",
        "    Loss_train = 0\n",
        "    MSE_train = 0\n",
        "\n",
        "    count_training=0\n",
        "\n",
        "    for x,y in zip(X[:54000], y_train[:54000]):\n",
        "\n",
        "      a,h,y_hat_training=feedforwardnn(x.reshape(784,1),num_layers,num_neurons,W,b,activation_function)\n",
        "\n",
        "      y_predicted=np.argmax(y_hat_training)\n",
        "\n",
        "      if(y==y_predicted):\n",
        "        count_training += 1\n",
        "\n",
        "\n",
        "      # Finding loss :\n",
        "      if(loss_method=='cross_entropy'):\n",
        "        if(y_hat_training[y]<=1e-6):\n",
        "          Loss_train = Loss_train\n",
        "\n",
        "        else:\n",
        "          Loss_train = Loss_train + -np.log(np.clip(y_hat_training[y], 1e-6, 400))\n",
        "\n",
        "        sum1=0.0\n",
        "\n",
        "        for i in range(1,len(W)):\n",
        "          sum1=sum1+(np.sum(W[i]**2))\n",
        "\n",
        "        regularized_term= (alpha / 2)* (sum1)\n",
        "\n",
        "        # Regularized Loss\n",
        "\n",
        "        Loss_train = Loss_train + regularized_term\n",
        "\n",
        "\n",
        "      # Finding Mean Squared Error:\n",
        "      else:\n",
        "        #making y_true one hot coded\n",
        "        y_true=np.zeros((num_classes))\n",
        "        y_true[y]=1\n",
        "\n",
        "        MSE_train= MSE_train + np.mean((y_true - y_hat_training) ** 2)\n",
        "\n",
        "    Training_Accuracy = count_training / 54000\n",
        "    print(f'Training Accuracy : {Training_Accuracy}')\n",
        "\n",
        "    if(loss_method=='cross_entropy'):\n",
        "      Loss_train = Loss_train / 54000\n",
        "\n",
        "      print(f'Training Loss : {Loss_train}')\n",
        "    else:\n",
        "\n",
        "      MSE_train = MSE_train / 54000\n",
        "\n",
        "      print(f'Training Mean squared error : {MSE_train}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # For validation error\n",
        "\n",
        "    #Initial loss is 0\n",
        "    Loss_val = 0\n",
        "    MSE_val = 0\n",
        "\n",
        "    count_validation=0\n",
        "    for x,y in zip(X[54000:], y_train[54000:]):\n",
        "\n",
        "      a,h,y_hat_validation=feedforwardnn(x.reshape(784,1),num_layers,num_neurons,W,b,activation_function)\n",
        "\n",
        "      y_predicted=np.argmax(y_hat_validation)\n",
        "\n",
        "      if(y==y_predicted):\n",
        "        count_validation += 1\n",
        "\n",
        "      # Finding loss :\n",
        "      if(loss_method=='cross_entropy'):\n",
        "\n",
        "        if(y_hat_validation[y]<=1e-6):\n",
        "          Loss_val = Loss_val\n",
        "\n",
        "        else:\n",
        "          Loss_val = Loss_val + -np.log(np.clip(y_hat_validation[y], 1e-6, 400))\n",
        "\n",
        "        sum2=0.0\n",
        "\n",
        "        for i in range(1,len(W)):\n",
        "          sum2=sum2+(np.sum(W[i]**2))\n",
        "\n",
        "        regularized_term= (alpha / 2) * (sum2)\n",
        "\n",
        "        # Regularized Loss\n",
        "\n",
        "        Loss_val = Loss_val + regularized_term\n",
        "\n",
        "      # Finding Mean Squared Error:\n",
        "\n",
        "      else:\n",
        "\n",
        "        #making y_true one hot coded\n",
        "        y_true=np.zeros((num_classes))\n",
        "        y_true[y]=1\n",
        "\n",
        "        MSE_val= MSE_val + np.mean((y_true - y_hat_validation) ** 2)\n",
        "\n",
        "\n",
        "\n",
        "    Validation_Accuracy = count_validation / 6000\n",
        "    print(f'Validation Accuracy : {Validation_Accuracy}')\n",
        "\n",
        "    if(loss_method=='cross_entropy'):\n",
        "\n",
        "      Loss_val = Loss_val / 6000\n",
        "\n",
        "      print(f'Validation Loss : {Loss_val}')\n",
        "\n",
        "    else:\n",
        "\n",
        "      MSE_val = MSE_val / 6000\n",
        "      print(f'Validation Mean squared error : {MSE_val}')\n",
        "      print(\"\\n\")\n",
        "\n",
        "    if(loss_method=='cross_entropy'):\n",
        "      wandb.log({'Training Accuracy': Training_Accuracy ,'Training Loss': Loss_train ,'Validation Accuracy': Validation_Accuracy ,'Validation Loss': Loss_val})\n",
        "    else:\n",
        "      wandb.log({'Training Accuracy': Training_Accuracy ,'Training Mean Squared Error': MSE_train ,'Validation Accuracy': Validation_Accuracy ,'Validation Mean Squared Error': MSE_val})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNPmlEB2DfKs"
      },
      "source": [
        "# Stochastic Gradient Descent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cp6Jp7OBDlXw"
      },
      "outputs": [],
      "source": [
        "def Stochastic_Gradient_descent(x_train,y_train,num_layers,num_neurons,n_features,num_classes,max_epochs,batch_size,alpha,activation_function,loss_method,eta,W,b):\n",
        "\n",
        "\n",
        "\n",
        "  X = x_train.reshape(x_train.shape[0], -1)\n",
        "\n",
        "\n",
        "\n",
        "  for i in (range(max_epochs)):\n",
        "    print(\"Epoch: %d\\n\"%(i+1))\n",
        "    wandb.log({'Epoch ': (i+1)})\n",
        "\n",
        "\n",
        "\n",
        "    for x,y in zip(X[:54000], y_train[:54000]):\n",
        "\n",
        "        dw = [np.zeros_like(w) for w in W]\n",
        "        db = [np.zeros_like(c) for c in b]\n",
        "\n",
        "        a,h,y_hat=feedforwardnn(x.reshape(784,1),num_layers,num_neurons,W,b,activation_function)\n",
        "        grad_with_w, grad_with_b = backward_propagation(x.reshape(784,1),h,a,W,b,y,y_hat,num_layers,num_classes,activation_function,loss_method)\n",
        "        for j in range(num_layers+1):\n",
        "\n",
        "          dw[j] = np.add(dw[j],grad_with_w[j])\n",
        "          db[j] = np.add(db[j],grad_with_b[j])\n",
        "\n",
        "        for j in range(num_layers+1):\n",
        "\n",
        "\n",
        "\n",
        "          W[j] = (W[j] - eta * grad_with_w[j] - eta * alpha * W[j])\n",
        "\n",
        "          b[j] = (b[j] - eta * grad_with_b[j] - eta * alpha * b[j])\n",
        "\n",
        "    # For accuracy loss and MSE\n",
        "\n",
        "    LossAccuracy(x_train,y_train,alpha,W,b,batch_size,activation_function,num_layers,n_features,num_classes,num_neurons,loss_method)\n",
        "\n",
        "  return W,b\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_3hTSqgD5Et"
      },
      "source": [
        "# Momentum based gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "E9SlEaN2EDbS"
      },
      "outputs": [],
      "source": [
        "def Momentum_Gradient_descent(x_train,y_train,num_layers,num_neurons,n_features,num_classes,max_epochs,batch_size,beta,alpha,activation_function,loss_method,eta,W,b):\n",
        "\n",
        "\n",
        "\n",
        "  X = x_train.reshape(x_train.shape[0], -1)\n",
        "\n",
        "\n",
        "  prev_uw = [np.zeros_like(w) for w in W]\n",
        "  prev_ub = [np.zeros_like(c) for c in b]\n",
        "\n",
        "  uw = [np.zeros_like(w) for w in W]\n",
        "  ub = [np.zeros_like(c) for c in b]\n",
        "\n",
        "\n",
        "\n",
        "  for i in (range(max_epochs)):\n",
        "    print(\"Epoch: %d\\n\"%(i+1))\n",
        "    wandb.log({'Epoch ': (i+1)})\n",
        "\n",
        "    num_points_seen = 0\n",
        "\n",
        "    dw = [np.zeros_like(w) for w in W]\n",
        "    db = [np.zeros_like(c) for c in b]\n",
        "\n",
        "\n",
        "\n",
        "    for x,y in zip(X[:54000], y_train[:54000]):\n",
        "\n",
        "\n",
        "        a,h,y_hat=feedforwardnn(x.reshape(784,1),num_layers,num_neurons,W,b,activation_function)\n",
        "        grad_with_w, grad_with_b = backward_propagation(x.reshape(784,1),h,a,W,b,y,y_hat,num_layers,num_classes,activation_function,loss_method)\n",
        "\n",
        "        for j in range(num_layers+1):\n",
        "\n",
        "          dw[j] = np.add(dw[j],grad_with_w[j])\n",
        "          db[j] = np.add(db[j],grad_with_b[j])\n",
        "\n",
        "\n",
        "        num_points_seen +=1\n",
        "\n",
        "        if(num_points_seen%batch_size == 0):\n",
        "\n",
        "          # Batch Normalization\n",
        "          for j in range(num_layers+1):\n",
        "\n",
        "            dw[j]=dw[j]/batch_size\n",
        "            db[j]=db[j]/batch_size\n",
        "\n",
        "\n",
        "\n",
        "          for j in range(num_layers+1):\n",
        "\n",
        "            uw[j] = beta * prev_uw[j] + eta * dw[j]\n",
        "            ub[j] = beta * prev_ub[j] + eta * db[j]\n",
        "\n",
        "            W[j] = (W[j] - eta * uw[j] - eta * alpha * W[j])\n",
        "\n",
        "            b[j] = (b[j] - eta * ub[j] - eta * alpha * b[j])\n",
        "\n",
        "            prev_uw[j] = uw[j]\n",
        "\n",
        "            prev_ub[j] = ub[j]\n",
        "\n",
        "          dw = [np.zeros_like(w) for w in W]\n",
        "          db = [np.zeros_like(c) for c in b]\n",
        "\n",
        "    # For accuracy loss and MSE\n",
        "    LossAccuracy(x_train,y_train,alpha,W,b,batch_size,activation_function,num_layers,n_features,num_classes,num_neurons,loss_method)\n",
        "\n",
        "  return W,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKfie50REEBV"
      },
      "source": [
        "# Nesterov accelerated gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YfhApE78ELDq"
      },
      "outputs": [],
      "source": [
        "def Nesterov_Gradient_descent(x_train,y_train,num_layers,num_neurons,n_features,num_classes,max_epochs,batch_size,beta,alpha,activation_function,loss_method,eta,W,b):\n",
        "\n",
        "\n",
        "  X = x_train.reshape(x_train.shape[0], -1)\n",
        "\n",
        "  prev_vw = [np.zeros_like(w) for w in W]\n",
        "  prev_vb = [np.zeros_like(c) for c in b]\n",
        "\n",
        "  vw = [np.zeros_like(w) for w in W]\n",
        "  vb = [np.zeros_like(c) for c in b]\n",
        "\n",
        "  W1 = [np.zeros_like(w) for w in W]\n",
        "  b1 = [np.zeros_like(c) for c in b]\n",
        "\n",
        "  for i in (range(max_epochs)):\n",
        "    print(\"Epoch: %d\\n\"%(i+1))\n",
        "    wandb.log({'Epoch ': (i+1)})\n",
        "\n",
        "    num_points_seen = 0\n",
        "\n",
        "    dw = [np.zeros_like(w) for w in W]\n",
        "    db = [np.zeros_like(c) for c in b]\n",
        "\n",
        "    for j in range(num_layers+1):\n",
        "      vw[j] = beta * prev_vw[j]\n",
        "      vb[j] = beta * prev_vb[j]\n",
        "\n",
        "    for x,y in zip(X[:54000], y_train[:54000]):\n",
        "\n",
        "\n",
        "        for j in range(num_layers+1):\n",
        "            W1[j] = W[j] - vw[j]\n",
        "            b1[j] = b[j] - vb[j]\n",
        "\n",
        "\n",
        "        a,h,y_hat=feedforwardnn(x.reshape(784,1),num_layers,num_neurons,W1,b1,activation_function)\n",
        "        grad_with_w, grad_with_b = backward_propagation(x.reshape(784,1),h,a,W,b,y,y_hat,num_layers,num_classes,activation_function,loss_method)\n",
        "        for j in range(num_layers+1):\n",
        "          dw[j] = np.add(dw[j],grad_with_w[j])\n",
        "          db[j] = np.add(db[j],grad_with_b[j])\n",
        "\n",
        "        num_points_seen += 1\n",
        "\n",
        "        if(num_points_seen%batch_size == 0):\n",
        "\n",
        "          # Batch Normalization\n",
        "          for j in range(num_layers+1):\n",
        "\n",
        "            dw[j]=dw[j]/batch_size\n",
        "            db[j]=db[j]/batch_size\n",
        "\n",
        "          for j in range(num_layers+1):\n",
        "\n",
        "            vw[j] = beta * prev_vw[j] + eta * dw[j]\n",
        "            vb[j] = beta * prev_vb[j] + eta * db[j]\n",
        "\n",
        "            W[j] = (W[j] - eta * vw[j] - eta * alpha * W[j])\n",
        "\n",
        "\n",
        "            b[j] = (b[j] - eta * vb[j] - eta * alpha * b[j])\n",
        "\n",
        "            prev_vw[j] = vw[j]\n",
        "\n",
        "            prev_vb[j] = vb[j]\n",
        "\n",
        "          # once batch is done initialize again\n",
        "\n",
        "          dw = [np.zeros_like(w) for w in W]\n",
        "          db = [np.zeros_like(c) for c in b]\n",
        "\n",
        "          for j in range(num_layers+1):\n",
        "            vw[j] = beta * prev_vw[j]\n",
        "            vb[j] = beta * prev_vb[j]\n",
        "\n",
        "    # For accuracy loss and MSE\n",
        "\n",
        "    LossAccuracy(x_train,y_train,alpha,W,b,batch_size,activation_function,num_layers,n_features,num_classes,num_neurons,loss_method)\n",
        "\n",
        "  return W,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nPInL-xELvw"
      },
      "source": [
        "# Rmsprop gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "shfelwMaEStJ"
      },
      "outputs": [],
      "source": [
        "def Rmsprop_Gradient_descent(x_train,y_train,num_layers,num_neurons,n_features,num_classes,max_epochs,batch_size,beta,eps,alpha,activation_function,loss_method,eta,W,b):\n",
        "\n",
        "\n",
        "\n",
        "  X = x_train.reshape(x_train.shape[0], -1)\n",
        "\n",
        "\n",
        "  vw = [np.zeros_like(w) for w in W]\n",
        "  vb = [np.zeros_like(c) for c in b]\n",
        "\n",
        "  for i in (range(max_epochs)):\n",
        "    print(\"Epoch: %d\\n\"%(i+1))\n",
        "    wandb.log({'Epoch ': (i+1)})\n",
        "\n",
        "    num_points_seen = 0\n",
        "\n",
        "\n",
        "    dw = [np.zeros_like(w) for w in W]\n",
        "    db = [np.zeros_like(c) for c in b]\n",
        "\n",
        "    for x,y in zip(X[:54000], y_train[:54000]):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        a,h,y_hat=feedforwardnn(x.reshape(784,1),num_layers,num_neurons,W,b,activation_function)\n",
        "        grad_with_w, grad_with_b = backward_propagation(x.reshape(784,1),h,a,W,b,y,y_hat,num_layers,num_classes,activation_function,loss_method)\n",
        "\n",
        "        for j in range(num_layers+1):\n",
        "          dw[j] = np.add(dw[j],grad_with_w[j])\n",
        "          db[j] = np.add(db[j],grad_with_b[j])\n",
        "\n",
        "\n",
        "        num_points_seen += 1\n",
        "\n",
        "        if(num_points_seen%batch_size == 0):\n",
        "\n",
        "\n",
        "          # Batch Normalization\n",
        "          for j in range(num_layers+1):\n",
        "\n",
        "            dw[j]=dw[j]/batch_size\n",
        "            db[j]=db[j]/batch_size\n",
        "\n",
        "\n",
        "          for j in range(num_layers+1):\n",
        "\n",
        "            vw[j] = beta * vw[j] + (1-beta) * (dw[j]**2)\n",
        "            vb[j] = beta * vb[j] + (1-beta) * (db[j]**2)\n",
        "\n",
        "            W[j] = (W[j] - (eta * dw[j]/(np.sqrt(vw[j])+eps)) - eta * alpha * W[j])\n",
        "\n",
        "\n",
        "            b[j] = (b[j] - (eta * db[j]/(np.sqrt(vb[j])+eps)) - eta * alpha * b[j])\n",
        "\n",
        "\n",
        "\n",
        "          dw = [np.zeros_like(w) for w in W]\n",
        "          db = [np.zeros_like(c) for c in b]\n",
        "\n",
        "    # For accuracy loss and MSE\n",
        "    LossAccuracy(x_train,y_train,alpha,W,b,batch_size,activation_function,num_layers,n_features,num_classes,num_neurons,loss_method)\n",
        "\n",
        "  return W,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTZmIi6NETJs"
      },
      "source": [
        "# Adam gradient descent :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RWdkwajHEb2D"
      },
      "outputs": [],
      "source": [
        "def Adam_Gradient_descent(x_train,y_train,num_layers,num_neurons,n_features,num_classes,max_epochs,batch_size,beta1,beta2,eps,alpha,activation_function,loss_method,eta,W,b):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  X = x_train.reshape(x_train.shape[0], -1)\n",
        "\n",
        "  mw = [np.zeros_like(w) for w in W]\n",
        "  mb = [np.zeros_like(c) for c in b]\n",
        "\n",
        "  mw_hat = [np.zeros_like(w) for w in W]\n",
        "  mb_hat = [np.zeros_like(c) for c in b]\n",
        "\n",
        "  vw = [np.zeros_like(w) for w in W]\n",
        "  vb = [np.zeros_like(c) for c in b]\n",
        "\n",
        "  vw_hat = [np.zeros_like(w) for w in W]\n",
        "  vb_hat = [np.zeros_like(c) for c in b]\n",
        "\n",
        "  for i in (range(max_epochs)):\n",
        "    print(\"Epoch: %d\\n\"%(i+1))\n",
        "    wandb.log({'Epoch ': (i+1)})\n",
        "\n",
        "    num_points_seen = 0\n",
        "\n",
        "    dw = [np.zeros_like(w) for w in W]\n",
        "    db = [np.zeros_like(c) for c in b]\n",
        "\n",
        "\n",
        "    for x,y in zip(X[:54000], y_train[:54000]):\n",
        "\n",
        "\n",
        "\n",
        "        a,h,y_hat=feedforwardnn(x.reshape(784,1),num_layers,num_neurons,W,b,activation_function)\n",
        "        grad_with_w, grad_with_b = backward_propagation(x.reshape(784,1),h,a,W,b,y,y_hat,num_layers,num_classes,activation_function,loss_method)\n",
        "\n",
        "        for j in range(num_layers+1):\n",
        "          dw[j] = np.add(dw[j],grad_with_w[j])\n",
        "          db[j] = np.add(db[j],grad_with_b[j])\n",
        "\n",
        "\n",
        "        num_points_seen += 1\n",
        "\n",
        "        if(num_points_seen%batch_size == 0):\n",
        "\n",
        "\n",
        "          # Batch Normalization\n",
        "          for j in range(num_layers+1):\n",
        "\n",
        "            dw[j]=dw[j]/batch_size\n",
        "            db[j]=db[j]/batch_size\n",
        "\n",
        "\n",
        "\n",
        "          for j in range(num_layers+1):\n",
        "\n",
        "            mw[j] = beta1 * mw[j] + (1-beta1) * dw[j]\n",
        "            mb[j] = beta1 * mb[j] + (1-beta1) * db[j]\n",
        "            vw[j] = beta2 * vw[j] + (1-beta2) * (dw[j]**2)\n",
        "            vb[j] = beta2 * vb[j] + (1-beta2) * (db[j]**2)\n",
        "\n",
        "            mw_hat[j] = mw[j] / (1-np.power(beta1,i+1))\n",
        "            mb_hat[j] = mb[j] / (1-np.power(beta1,i+1))\n",
        "            vw_hat[j] = vw[j] / (1-np.power(beta2,i+1))\n",
        "            vb_hat[j] = vb[j] / (1-np.power(beta2,i+1))\n",
        "\n",
        "            W[j] = (W[j] -(eta * mw_hat[j]/(np.sqrt(vw_hat[j])+eps)) - eta * alpha * W[j])\n",
        "\n",
        "\n",
        "            b[j] = (b[j] -(eta * mb_hat[j]/(np.sqrt(vb_hat[j])+eps)) - eta * alpha * b[j])\n",
        "\n",
        "\n",
        "          dw = [np.zeros_like(w) for w in W]\n",
        "          db = [np.zeros_like(c) for c in b]\n",
        "\n",
        "    # For accuracy loss and MSE\n",
        "\n",
        "    LossAccuracy(x_train,y_train,alpha,W,b,batch_size,activation_function,num_layers,n_features,num_classes,num_neurons,loss_method)\n",
        "\n",
        "  return W,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWiWeTtcEcuG"
      },
      "source": [
        "# Nadam gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wAEDgt7gEjn5"
      },
      "outputs": [],
      "source": [
        "def Nadam_Gradient_descent(x_train,y_train,num_layers,num_neurons,n_features,num_classes,max_epochs,batch_size,beta1,beta2,eps,alpha,activation_function,loss_method,eta,W,b):\n",
        "\n",
        "\n",
        "\n",
        "  X = x_train.reshape(x_train.shape[0], -1)\n",
        "\n",
        "  mw = [np.zeros_like(w) for w in W]\n",
        "  mb = [np.zeros_like(c) for c in b]\n",
        "\n",
        "  mw_hat = [np.zeros_like(w) for w in W]\n",
        "  mb_hat = [np.zeros_like(c) for c in b]\n",
        "\n",
        "  vw = [np.zeros_like(w) for w in W]\n",
        "  vb = [np.zeros_like(c) for c in b]\n",
        "\n",
        "  vw_hat = [np.zeros_like(w) for w in W]\n",
        "  vb_hat = [np.zeros_like(c) for c in b]\n",
        "\n",
        "  for i in (range(max_epochs)):\n",
        "\n",
        "    print(\"Epoch: %d\\n\"%(i+1))\n",
        "    wandb.log({'Epoch ': (i+1)})\n",
        "\n",
        "    num_points_seen = 0\n",
        "\n",
        "    dw = [np.zeros_like(w) for w in W]\n",
        "    db = [np.zeros_like(c) for c in b]\n",
        "\n",
        "\n",
        "    for x,y in zip(X[:54000], y_train[:54000]):\n",
        "\n",
        "\n",
        "\n",
        "        a,h,y_hat=feedforwardnn(x.reshape(784,1),num_layers,num_neurons,W,b,activation_function)\n",
        "        grad_with_w, grad_with_b = backward_propagation(x.reshape(784,1),h,a,W,b,y,y_hat,num_layers,num_classes,activation_function,loss_method)\n",
        "        for j in range(num_layers+1):\n",
        "          dw[j] = np.add(dw[j],grad_with_w[j])\n",
        "          db[j] = np.add(db[j],grad_with_b[j])\n",
        "\n",
        "\n",
        "        num_points_seen += 1\n",
        "\n",
        "        if(num_points_seen%batch_size == 0):\n",
        "\n",
        "          # Batch Normalization\n",
        "          for j in range(num_layers+1):\n",
        "\n",
        "            dw[j]=dw[j]/batch_size\n",
        "            db[j]=db[j]/batch_size\n",
        "\n",
        "          for j in range(num_layers+1):\n",
        "\n",
        "            mw[j] = beta1 * mw[j] + (1-beta1) * dw[j]\n",
        "            mb[j] = beta1 * mb[j] + (1-beta1) * db[j]\n",
        "            vw[j] = beta2 * vw[j] + (1-beta2) * (dw[j]**2)\n",
        "            vb[j] = beta2 * vb[j] + (1-beta2) * (db[j]**2)\n",
        "\n",
        "            mw_hat[j] = mw[j] / (1-np.power(beta1,i+1))\n",
        "            mb_hat[j] = mb[j] / (1-np.power(beta1,i+1))\n",
        "            vw_hat[j] = vw[j] / (1-np.power(beta2,i+1))\n",
        "            vb_hat[j] = vb[j] / (1-np.power(beta2,i+1))\n",
        "\n",
        "            W[j] = (W[j] -(eta /(np.sqrt(vw_hat[j])+eps)) * (beta1 * mw_hat[j] + (1-beta1)*dw[j]/(1-beta1**(i+1))) - eta * alpha * W[j])\n",
        "\n",
        "\n",
        "            b[j] = (b[j] -(eta/(np.sqrt(vb_hat[j])+eps)) * (beta1 * mb_hat[j] + (1-beta1)*db[j]/(1-beta1**(i+1))) - eta * alpha * b[j])\n",
        "\n",
        "\n",
        "          dw = [np.zeros_like(w) for w in W]\n",
        "          db = [np.zeros_like(c) for c in b]\n",
        "\n",
        "    # For accuracy loss and MSE\n",
        "\n",
        "    LossAccuracy(x_train,y_train,alpha,W,b,batch_size,activation_function,num_layers,n_features,num_classes,num_neurons,loss_method)\n",
        "\n",
        "  return W,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSBWOPpQElQX"
      },
      "source": [
        "# Main Function calling methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tCMVkmUVNrZi"
      },
      "outputs": [],
      "source": [
        "def arguments(num_hidden_layers,num_neurons,activation_function,initialize_method,max_epochs,batch_size,eta,alpha,beta,beta1,beta2,eps,Gradient_descent_method,loss_method):\n",
        "\n",
        "\n",
        "  (x_train,y_train),(x_test,y_test)= fashion_mnist.load_data()\n",
        "  x_train=x_train/255\n",
        "  x_test=x_test/255\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "  # NOTE: TOTAL NUMBER OF LAYERS\n",
        "\n",
        "  num_layers = num_hidden_layers + 1\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "  num_classes = 10\n",
        "\n",
        "\n",
        "  X = x_train\n",
        "\n",
        "  # Reshaping (60000,28,28) to (60000,784)\n",
        "  X = X.reshape(X.shape[0], -1)\n",
        "\n",
        "\n",
        "  n_features=X.shape[1]\n",
        "\n",
        "  # # Choose number of nuerons per each hidden layer\n",
        "  # num_neurons= 128\n",
        "\n",
        "  # # Choose activation funtion\n",
        "\n",
        "\n",
        "\n",
        "  # activation_function = \"ReLU\"\n",
        "\n",
        "  # #Weights and bias initialization\n",
        "\n",
        "\n",
        "\n",
        "  # initialize_method ='Xavier'\n",
        "\n",
        "  W,b = initializeWeightandbias(initialize_method,num_layers,num_neurons,n_features,num_classes)\n",
        "\n",
        "\n",
        "\n",
        "  # max_epochs=5\n",
        "  # batch_size=64\n",
        "\n",
        "\n",
        "\n",
        "  # eta=0.001\n",
        "\n",
        "  # alpha=0.0005\n",
        "\n",
        "  # #For Momentum Gradient Descent\n",
        "\n",
        "  # beta=0.9\n",
        "  # beta1=0.9\n",
        "  # beta2 = 0.9\n",
        "\n",
        "  # eps = 1e-4\n",
        "\n",
        "\n",
        "  # # Choose Gradient descent method\n",
        "\n",
        "\n",
        "  # Gradient_descent_method = 'momentum'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if(Gradient_descent_method=='sgd'):\n",
        "    W, b = Stochastic_Gradient_descent(x_train,y_train,num_layers,num_neurons,n_features,num_classes,max_epochs,batch_size,alpha,activation_function,loss_method,eta,W,b)\n",
        "  elif(Gradient_descent_method=='momentum'):\n",
        "    W, b = Momentum_Gradient_descent(x_train,y_train,num_layers,num_neurons,n_features,num_classes,max_epochs,batch_size,beta,alpha,activation_function,loss_method,eta,W,b)\n",
        "  elif(Gradient_descent_method=='nesterov'):\n",
        "    W, b = Nesterov_Gradient_descent(x_train,y_train,num_layers,num_neurons,n_features,num_classes,max_epochs,batch_size,beta,alpha,activation_function,loss_method,eta,W,b)\n",
        "  elif(Gradient_descent_method=='rmsprop'):\n",
        "    W, b = Rmsprop_Gradient_descent(x_train,y_train,num_layers,num_neurons,n_features,num_classes,max_epochs,batch_size,beta,eps,alpha,activation_function,loss_method,eta,W,b)\n",
        "  elif(Gradient_descent_method=='adam'):\n",
        "    W, b = Adam_Gradient_descent(x_train,y_train,num_layers,num_neurons,n_features,num_classes,max_epochs,batch_size,beta1,beta2,eps,alpha,activation_function,loss_method,eta,W,b)\n",
        "  elif(Gradient_descent_method=='nadam'):\n",
        "    W, b = Nadam_Gradient_descent(x_train,y_train,num_layers,num_neurons,n_features,num_classes,max_epochs,batch_size,beta1,beta2,eps,alpha,activation_function,loss_method,eta,W,b)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACty1RhXkOSi",
        "outputId": "ef5a061e-196b-4d78-cd8f-7487ad7c355a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "1N8Q9-sQkhXS",
        "outputId": "93c34b58-0d70-4150-b8a5-c961aa0a42b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'sweep_cross_entropy_and_MSE',\n",
        "    'metric': {\n",
        "        'name': 'Validation Accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5]\n",
        "        },\n",
        "        'hidden_layers': {\n",
        "            'values': [3, 4, 5]\n",
        "        },\n",
        "        'hidden_layer_size': {\n",
        "            'values': [32, 64]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0, 0.0005, 0.5]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [0.001, 0.0001]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [16, 32, 64]\n",
        "        },\n",
        "        'weight_initialization': {\n",
        "            'values': ['random', 'Xavier']\n",
        "        },\n",
        "        'activation_function': {\n",
        "            'values': ['sigmoid', 'tanh', 'ReLU']\n",
        "        },\n",
        "        'loss': {\n",
        "            'values': ['cross_entropy']\n",
        "        }\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='DL_Assignment_1_CS23M046')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_a3dRrjk0J6",
        "outputId": "ca993cd4-92be-4412-cce7-439debef98a5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: ktz22oxj\n",
            "Sweep URL: https://wandb.ai/cs23m046/DL_Assignment_1_CS23M046/sweeps/ktz22oxj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    '''\n",
        "    WandB calls main function each time with differnet combination.\n",
        "\n",
        "    We can retrive the same and use the same values for our hypermeters.\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    with wandb.init() as run:\n",
        "        config=wandb.config\n",
        "        run_name=\"hl_\"+str(config.hidden_layers)+\"_bs_\"+str(config.batch_size)+\"_ac_\"+config.activation_function\n",
        "        wandb.run.name=run_name\n",
        "        arguments(config.hidden_layers,config.hidden_layer_size,config.activation_function,config.weight_initialization,config.epochs,config.batch_size,config.learning_rate,config.weight_decay,0.9,0.9,0.999,1e-8,config.optimizer,config.loss)\n",
        "        #obj=NN(wandb.config['num_layers'],wandb.config['hidden_size'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "wandb.agent(sweep_id, function=main,count=10) # calls main function for count number of times.\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "CnI4t4CwxqCs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzkSN6GsWn00Q6ScUz3FU0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}